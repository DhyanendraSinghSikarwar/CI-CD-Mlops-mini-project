1. use cookiecutter for template
2. update/delete the files and folder if require inside src
3. initializing the git at local
3. create git hub repo with same name and add remote repo with local and push the existing files 
4. connect dagshub with github repo and copy the experiment_id and other required code
5. create the dagshub_setup.py inside notebooks folder and setup dagshub
-------plan of project---
we have tweet emotion data,first we will apply BoW and TfIdf with algoriths like logisticRegression,
RandomForest, Xgb, etc which ever combination give good result, we take that combination and run
hyperparameter tuining on that combination then use dvc pipeline and register the model.
exp1- Bow + algos 
exp2- TfIdf + algos
exp3- hyperparameter tuning with best combination
These experiments will be written in jupyterNotebook and tracked by dagshub

6. In exp-1 , we used jupyternotebook to perform the experiment and tracked in dagshub using mlflow. here we run the logisticRegression Baseline model
7. In exp-2 , we perform apply Bow and Tfidf as preprocessing and apply different models and find Bow and LoR is having better accuracy(f1 score).
8. In exp-3, we  choose best combination and apply hyperparameter tuning at this combination. although, we can try hyperparameter tuning with all combination

Now, we will create DVC pipeline along with best combiination model
9. dvc init
10. Adding dvc remote (local/s3), command is : dvc remote add -d myremote <path>
11.  create a DVC pipeline
    - data ingestion
    - data preprocessing
    - feature engineering
    - model building
    - model evaluation
12. add dvc.yaml and params.yaml file and then run command to execute the pipeline "dvc repro"
13. we are also logging the model in model_evaluation file in dagshub in each run, along with those experiments
14. setup AWS s3 to store the dataset or data versions store
     - pip install dvc[s3] awscli 
     - dvc remote list   # to check avaialbe stores
     - dvc remote remove <remote_name>
     - aws configure  # to configure the aws

15. Run the DVC pipeline using "dvc repro". The best model is logged to DagsHub/MLflow as a run artifact (not registered in the Model Registry).
16. In the model registration stage, tag the MLflow run as "production" (using client.set_tag(run_id, "model_status", "production")). Do not use Model Registry APIs, as DagsHub does not support them.
17. Create the Flask app. In the app, fetch the latest model artifact from the MLflow run tagged as "production" (using MLflowClient search_runs and load_model). Do not use Model Registry APIs.
18. For serving in production (EC2, etc.), always use a WSGI server (e.g., gunicorn) and ensure the app loads the model from the latest "production" run artifact.

# Create the CI pipeline 
19. Add the dvc pipeline via github action (via CI), currently we are using authentication via 'dagshub.init' but it requires manual authentication
So we can access dagshub using secrets, so we update the code

    dagshub_token = os.getenv("DAGSHUB_PAT")
    if not dagshub_token:
        raise EnvironmentError("DAGSHUB_PAT environment variable is not set")

    os.environ["MLFLOW_TRACKING_USERNAME"] = dagshub_token
    os.environ["MLFLOW_TRACKING_PASSWORD"] = dagshub_token

20. Now create the secret/token from dagshub and add this to github -> settings->secrets and variables->actions-> new repository secret
21. update the ci.yaml file 
    name: Run DVC pipeline
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: dvc repro

# Best Practices and Notes
- Do not use MLflow Model Registry APIs on DagsHub; use run tags for model promotion.
- Always load the model for inference from the MLflow run artifact tagged as "production".
- Use a requirements.txt or conda.yaml to ensure consistent environments for training and serving.
- For production, use a WSGI server (not Flask's built-in server).