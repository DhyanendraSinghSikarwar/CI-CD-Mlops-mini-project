1. use cookiecutter for template
2. update/delete the files and folder if require inside src
3. initializing the git at local
3. create git hub repo with same name and add remote repo with local and push the existing files 
4. connect dagshub with github repo and copy the experiment_id and other required code
5. create the dagshub_setup.py inside notebooks folder and setup dagshub
-------plan of project---
we have tweet emotion data,first we will apply BoW and TfIdf with algoriths like logisticRegression,
RandomForest, Xgb, etc which ever combination give good result, we take that combination and run
hyperparameter tuining on that combination then use dvc pipeline and register the model.
exp1- Bow + algos 
exp2- TfIdf + algos
exp3- hyperparameter tuning with best combination
These experiments will be written in jupyterNotebook and tracked by dagshub

6. In exp-1 , we used jupyternotebook to perform the experiment and tracked in dagshub using mlflow. here we run the logisticRegression Baseline model
7. In exp-2 , we perform apply Bow and Tfidf as preprocessing and apply different models and find Bow and LoR is having better accuracy(f1 score).
8. In exp-3, we  choose best combination and apply hyperparameter tuning at this combination. although, we can try hyperparameter tuning with all combination

Now, we will create DVC pipeline along with best combiination model
9. dvc init
10. Adding dvc remote (local/s3), command is : dvc remote add -d myremote <path>
11.  create a DVC pipeline
    - data ingestion
    - data preprocessing
    - feature engineering
    - model building
    - model evaluation
12. add dvc.yaml and params.yaml file and then run command to execute the pipeline "dvc repro"
13. we are also logging the model in model_evaluation file in dagshub in each run, along with those experiments
14. setup AWS s3 to store the dataset or data versions store
     - pip install dvc[s3] awscli 
     - dvc remote list   # to check avaialbe stores
     - dvc remote remove <remote_name>
     - aws configure  # to configure the aws

15. Run the DVC pipeline using "dvc repro". The best model is logged to DagsHub/MLflow as a run artifact (not registered in the Model Registry).
16. In the model registration stage, tag the MLflow run as "production" (using client.set_tag(run_id, "model_status", "production")). Do not use Model Registry APIs, as DagsHub does not support them.
17. Create the Flask app. In the app, fetch the latest model artifact from the MLflow run tagged as "production" (using MLflowClient search_runs and load_model). Do not use Model Registry APIs.
18. For serving in production (EC2, etc.), always use a WSGI server (e.g., gunicorn) and ensure the app loads the model from the latest "production" run artifact.

# Create the CI pipeline 
19. Add the dvc pipeline via github action (via CI), currently we are using authentication via 'dagshub.init' but it requires manual authentication
So we can access dagshub using secrets, so we update the code

    dagshub_token = os.getenv("DAGSHUB_PAT")
    if not dagshub_token:
        raise EnvironmentError("DAGSHUB_PAT environment variable is not set")

    os.environ["MLFLOW_TRACKING_USERNAME"] = dagshub_token
    os.environ["MLFLOW_TRACKING_PASSWORD"] = dagshub_token

20. Now create the secret/token from dagshub and add this to github -> settings->secrets and variables->actions-> new repository secret
21. update the ci.yaml file 
    name: Run DVC pipeline
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: dvc repro

# Best Practices and Notes
- Do not use MLflow Model Registry APIs on DagsHub; use run tags for model promotion.
- Always load the model for inference from the MLflow run artifact tagged as "production".
- Use a requirements.txt or conda.yaml to ensure consistent environments for training and serving.
- For production, use a WSGI server (not Flask's built-in server).

22. Now, we need to dockerize the project either the whole project or the part of it (like flask app only for prediction)
  Here, we will dockerize the flask_app folder only, but it also require vector.pkl file and few libraries
23. Since, current requirements.txt contain libraries required for whole project. So we will use "pipreqs" to get 
  the required libraries to run particular folder and create the subset of requirements.txt file
"pip install pipreqs"
24. go to the flask_app/ folder and run "pipreqs . --force" this will generate separate requirements.txt inside flask_app
25. we will add host="0.0.0.0" to access this application outside docker image
if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0")
26. Now, create dockerfile in root directory and bulid the docker image from root directory
 command used: "docker build -t dhyanendra007/emotion1 ." you can change the name <dhyanendra007/emotion1>
27. Now we will run the docker but we don't have the "DAGSHUB_PAT" key which is required in app.py
 So, we can send this in the command
 "docker run -p 8888:5000 -e DAGSHUB_PAT=<5e327750478b72352ed039dd541f17fc2d225a26> dhyanendra007/emotion1"
28. Now to automate the docker , we will add 3 steps in ci.yaml
   1. login to dockerhub
   2. building the image --> dockerfile
   3. push to dockerhub
Steps to add in ci.yaml file:
      - name: Log in to Docker Hub
        if: success()
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build Docker image
        if: success()
        run: |
          docker build -t ${{ secrets.DOCKER_USERNAME }}/emotion2:latest .

      - name: Push Docker image to Docker Hub
        if: success()
        run: |
          docker push ${{ secrets.DOCKER_USERNAME }}/emotion2:latest
29. First create the token and give read and write access from docker hub then add them in github secrets 
    DOCKER_USERNAME : <name of the user>
    DOCKER_PASSWORD : <token generated from docker hub>
30. Now push the code to github, then 
  1. first pipeline will excecute which generate the model and pushed to dagshub using dvc.yaml
  2. model will tag as production
  3. run model test 
  4. take model from dagshub and perform flask test
  5. login to docker , create the image and pushed to docker hub
31. Now start the deployment:

To run the command in EC2 instance to install docker
    1 sudo apt-get update
    2  sudo apt-get install -y docker.io
    3  sudo systemctl start docker
    4  sudo systemctl enable docker
    5  sudo docker pull dhyanendra007/emotion2
Now run the below command:
"sudo docker run -p 8888:5000 -e DAGSHUB_PAT=5e327750478b72352ed039dd541f17fc2d225a26 dhyanendra007/emotion2"
and add the security group to ec2 instance and allow inbound traffic port 8888 or as mentioned in the above command and use public DNS with http
32. Now to integrate this deployment in CICD pipeline:
   1. first, we need to run all above command in ec2
   2. then run one command to not to use sudo everytime:"sudo usermod -aG docker ubuntu" since ubuntu is user selected while creating that instance
   3. update the ci.yaml file:
   """
   - name: Deploy to EC2
        if: success()
        uses: appleboy/ssh-action@v0.1.8
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          port: ${{ secrets.EC2_PORT }}
          script: |
            docker pull ${{ secrets.DOCKER_USERNAME }}/emotion2:latest
            docker stop my-app || true      # this will stop my-app container is already running (if no container present then True will handle the error)
            docker rm my-app || true
            docker run -d -p 80:80 --name my-app \        # --name my-app this will provide the name as my-app to the container
              -e DAGSHUB_PAT=${{ secrets.DAGSHUB_PAT }} \
              ${{ secrets.DOCKER_USERNAME }}/emotion2:latest 
  """
  4. Add EC2_HOST, EC2_USER, EC2_SSH_KEY, EC2_PORT in github secrets variables
  5. Now commit the changes and push
            